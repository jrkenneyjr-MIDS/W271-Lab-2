---
title: "Lab 02"
date: "Oct 12, 2017"
output: pdf_document
fontsize: 11pt
geometry: margin=0.75in
---
#SECTION - 3  STATISTICAL MODELING:
Section 3: Statistical Modeling. Start the section summarizing the key results - what variables, if any, are the key predictors of the year 2016 contribution? What are the key techniques you have experimented? What method did you use in your final model? How did you choose the final model? What model performance criteria did you use to choose the final model? What statistical infernece did you perform? Explain them. Comment on statistical significance vs. economic significance.

##3.1 KEY RESULTS:  
We found that the best way to predict the level of contribution in 2016 for a particular person is to look at their level of contribution in 2015 and how often they have given in the past. Here is why we think this makes sense: we believe alumni-giving would be influenced by factors like connectedness to the university (Alumni event attendance, proximity to univesity), affluence (utilizing proxy measures like class year, major, next degree, marital status), exposure to marketing materials about the university, news related to the university etc.   However many of these factors do not change much (or at all) in the time horizon for which we were provided data (even if they changed, we were not provided the change information) - which basically means that we should be able to witness the influence of these factors in past years' giving information. Said another way these factors combined to result in a certain pattern of giving in each of the years 2012 - 2015;  so when we want to predict 2016 giving, we can simply use the past giving which encapsulates all the factors that influence someone to give.  

The functional form of our chosen model for predicting 2016 is:  

$log(\frac{\hat{\pi}_{j}}{\hat{\pi}_{base}}) = \beta_{0j} +\sum_{k=1}^{4}\beta_{kj} \cdot FY15_{k}+\beta_{4j}\cdot YearsGiven$  

where $FY15_k$ is the category of giving in 2015; and $YearsGiven$ is the number of years (between 2012 and 2015) that the person contributed; $\hat{\pi}_j$ is the probability of being in the $j^{th}$ category; $\hat{\pi}_{base}$ is the probability of being the base category (in our case this is the [0,1) category or "non-givers").  

We went with a multinomial logistic regression technique because the target variable is multinomial.  Let's run through the assumptions behind a multinomial logistic regression to make sure that it is applicable here:  
1. The target variable is a categorical variable
2. While we do not have additional information about the exact sampling technique employed, since it is mentione that it is a carefully selected sample - we assume that it is an independent, identically distributed sample.
3. No perfect collinearity (we will address this by the choice of dependent variables)
4. Linearity - log odds of the the dependent variable is linearly related to the independent variables (this holds by the structural assumption)
5. No complete separation (we don't observe any, but we can also rely on the R software to identify this for us)

Please find the model details (including coefficients for all categories) below under the "CHOSEN MODEL" heading.  We provide below the functional equation for the log (odds) for the second category of giving levels:  

$log(\frac{\hat{\pi}_{[1,100)}}{\hat{\pi}_{[0,1)}}) = -2.87 + 1.10 \cdot FY15[1,100) -0.65\cdot FY15[100,250)-15.24\cdot FY15[250,500)-12.49\cdot FY15[500,20000) + 0.8\cdot YearsGiven$  

##3.1.1 MODEL INTERPRETATION  
The above equation shows the relationship between the log of Odds of being in a specific giving-category in 2016 vs. being in the baseline category of not-giving as a  linear function of giving-category in 2015 and number of years given in the past.  Let's interpret the coefficient for the "years given" variable. This basically says that for every 1-year increase in the number of years in the past when a contribution was made, the probability of giving in the [1,100) category in 2016 increases $e^{0.8} = 2.2$ times or nearly doubles. 

##3.2 MODELING APPROACH, OTHER MODELS AND TECHNIQUES ATTEMPTED:  
The problem as stated was to predict the category of giving in 2016.  We chose to solve this as a $\hat{Y}$ problem - and divide the data into a training set that we would use to fit the model and a test set that we could use to evaluate the model.  
However we thought that while such a model might be effective in predicting what levels of giving category a person would belong to in 2016, it might inform us on the key influencers of giving. To truly understand the key drivers and answer questions like "does attending an event increase the likelihood of giving?" - we need to solve the $\hat{\beta}$ problem.  So in order to answer those questions - we developed a model of the functional form:

$log(\frac{\hat{\pi}_{j}}{\hat{\pi}_{base}}) = \beta_{0j} + \beta_{1j}\cdot NextDeg + \beta_{2j}\cdot Event+ \beta_{3j}\cdot Class + \beta_{4j}\cdot MS + \beta_{5j}\cdot Gender$  

where $NextDeg$ = indicator for whether a person has an additional degree or not.    
$Event$ = indicator variable for whether someone attended an alumni event  
$Class$ = categorical variable representing the class year decade  
$MS$ = Categorical varable representing Marital Status  
$Gender$ = categorical variable indicating Male or Female.  

Please find the model details (including coefficients for all categories) below under the "BETA HAT MODEL" heading.  We provide below the functional equation for the log (odds) for the second category of giving levels:  

$log(\frac{\hat{\pi}_{[1,100)}}{\hat{\pi}_{[0,1)}}) = -2.3 + 0.98\cdot NextDeg + 0.24\cdot Event+0.03\cdot Class1992 +0.47\cdot Class2002 + 0.91 \cdot Class2012 + 0.19\cdot StatusMarried -0.14 StatusSingle + 1.8\cdot StatusWidowed -0.43\cdot Male$  

Interpretation:
Based on the coefficient values above, we can say that all else being equal, attending an alumni event changes the odds (relative to not giving) that someone would give in the [1,100) level by $e^0.24 = 1.27$ times or increases the odds by 27%;  Interpreting the Marital Status coefficient - we can say that being single reduces the odds (relative to not giving) of being a category [1,100) giver by 13% ($1-e^{-0.14} = 0.13$) compared to being Divorced (which is the baseline category for Marital Status).  

##3.3 MODEL EVALUATION, SELECTION & STATISTICAL INFERENCE:
We tried several functional forms of the basic model:  
1. model.fit2a: Simple model that just included FY2015 giving category.  This one did not do as well because we are not including any historical information - someone who gave for 3 years in a row, and not in 2015 is more likely to give in 2016 than not.  

2. model.fit2b: Included all the historical yearly giving categories (which did not perform well because the dependent variables were very correlated resulting in very high standard errors on the coefficients, and poor prediction performacne), 
one which included 

3. model.fit2c: our chosen model

4. model.fit2d: included FY2015 category, Years given, and a few other variables like alumni event attendance, next degree and gender;  The coeff. for the event attendance variable was not statistically significant;  but the other variables were. However our prediction accuracy was not as good as our chosen model (77% for this model vs. 80% for the chosen model).  The BIC for our chosen model was 1262 compared to 1307 for this model.  

5.  model.fit3a: This is our $\hat{\beta}$ model that we described above.  It attempts to quantify the influence of certain variables in the absence of historica information.  We like this model as a way to educate marketing and other outreach efforts as opposed to using it as a predictor.  

6.  model.fit4a: We tried an Ordinal model - since the categories are ordered.  However this model did not produce a higher prediction accuracy.  We think this is because the proportional odds model results in the same $\beta_k$ values for all categories;  this is a slightly less flexible model compared to the multinomial model and as such doesn't perform as well.  


##3.5 STATISTICAL SIGNIFICANCE VS. ECONOMIC SIGNIFICANCE


```{r message=FALSE, warning=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
#Libraries required 
library(car)
library(dplyr)
library(gridExtra)
library(effsize) #Used to calculate Cohen's D for T-Test
library(aod)    #Used for effect size of the logit model
library(mcprofile) #Used for confidence intervals
library(package = MASS)  # Location of parcoord() function
library(vcd)
library(data.table)
library(stargazer)
library(nnet)
library(ordinal)
library(caret)

#can delete this line below if dataset is already in workspace
#otherwise, run SECTION1.Rmd file (need this process of storing the data frame and
#loading it again if we knit the SECTION files separately)

load("dt_dataframe.Rda")

#Renaming to shorter names - to fit display
names(dt)[names(dt)=="FY15GivingCat"] = "FY15"
names(dt)[names(dt)=="FY14GivingCat"] = "FY14"
names(dt)[names(dt)=="FY13GivingCat"] = "FY13"
names(dt)[names(dt)=="FY12GivingCat"] = "FY12"
names(dt)[names(dt)=="AttendenceEvent"] = "Event"
names(dt)[names(dt)=="ClassYearCat"] = "Class"
names(dt)[names(dt)=="Marital.Status"] = "MS"
names(dt)[names(dt)=="NextDegCat"] = "NextDeg"


```


```{r}
set.seed(1234)
train_index = createDataPartition(dt$FY16GivingCat, p=0.8, list=FALSE, times=1)
dt_train = dt[train_index]
dt_test = dt[-train_index]
PredAcc = function(data_set, mod){  #data_set: a data frame with data; mod: model fit on data
  dep = names(mod$model[1])  #Name of Dependent Variable
  act = data_set[, ..dep]   #List of Actual Observed Outcomes
  if (class(mod)[1] == "glm"){
    prob = predict(mod, newdata = data_set, type = "response") #Probability Binary is True
    pred = ifelse(prob>0.5, 1, 0)       #Predicted Result
  }else if (class(mod)[1] == "multinom"){
    prob = predict(mod, newdata = data_set, type = 'probs')  #Probability of each category
    pred = colnames(prob)[max.col(prob, ties.method = "random")]  #Cat of highest probability
  }else if (class(mod)[1] == 'clm'){
    pred = predict(mod, newdata = data_set, type = 'class')  #Cat of highest probability
  }
  results = data.frame(predicted = pred, actual = act)
  confusionMatrix(results[,1],results[,2], positive = '1') #Analysis of Prediction Accuracy Stats
}
Accuracy= function(CM_train, CM_test, model_name, mod){
  data.frame(row.names = model_name, TrainAcc = round(CM_train$overall[1],2), 
                   TestAcc = round(CM_test$overall[1],2), modelspec = mod)
}
PrintModel=function(mod, disp_se=TRUE){ # print pretty 
  x = summary(mod)
  coefs = round(x$coefficients,2)
  print((x$call)$formula)
  print("coeffs")
  print(coefs)
  if (class(mod)!="clm"){
    if (disp_se == TRUE){
      stderr = round(x$standard.errors,2)
      print("standard errors")
      print(stderr)
    }
  }
}
```
##modelfit2a, modelfit2b:  
```{r message=FALSE}
model.fit2a = multinom(FY16GivingCat~FY15, data=dt_train, model = TRUE, trace=FALSE)
#summary(model.fit2a)
PrintModel(model.fit2a, FALSE)
Anova(model.fit2a, test="LR", trace=FALSE)
CMTr = PredAcc(dt_train, mod=model.fit2a)
CMTe = PredAcc(dt_test, mod=model.fit2a)
s = as.character(strsplit(as.character(model.fit2a$call$formula),"~")[3])
ac_table = Accuracy(CMTr, CMTe, "model.fit2a",s)
#Naive model - just use FY15 category to predict 2016 - we should be able to do better than this
pred_val = dt_train$FY15
act_val = dt_train$FY16GivingCat
results = data.frame(predicted=pred_val, actual=act_val)
CMTr = confusionMatrix(results[,1],results[,2], positive='1')
pred_val = dt_test$FY15
act_val = dt_test$FY16GivingCat
results = data.frame(predicted=pred_val, actual=act_val)
CMTe = confusionMatrix(results[,1],results[,2], positive='1')
s = "FY15"
ac_table = rbind(ac_table,Accuracy(CMTr, CMTe, "naive_model",s))
#uSe all historical giving categories (massive collinearity) 
model.fit2b = multinom(FY16GivingCat~FY15+FY14+FY13+FY12, data=dt_train, model = TRUE, trace=FALSE)
PrintModel(model.fit2b)
Anova(model.fit2b, test="LR")
CMTr = PredAcc(dt_train, mod=model.fit2b)
CMTe = PredAcc(dt_test, mod=model.fit2b)
s = as.character(strsplit(as.character(model.fit2b$call$formula),"~")[3])
ac_table = rbind(ac_table,Accuracy(CMTr,CMTe,"model.fit2b",s))
```
##CHOSEN MODEL
```{r message=FALSE, warning=FALSE}
#Address collinearity by using 2015 giving category & yearsgiven instead of each year's cate
model.fit2c = multinom(FY16GivingCat~FY15+YearsGiven, data=dt_train, model=TRUE, trace=FALSE)
PrintModel(model.fit2c)
Anova(model.fit2c, test="LR")
CMTr = PredAcc(dt_train, mod=model.fit2c)
CMTe = PredAcc(dt_test, mod=model.fit2c)
s = as.character(strsplit(as.character(model.fit2c$call$formula),"~")[3])
ac_table= rbind(ac_table, Accuracy(CMTr, CMTe, "model.fit2c", s))
#Add other variables that seemed significant in the EDA.
```
##modelfit.2d
```{r message=FALSE, warning=FALSE}
model.fit2d = multinom(FY16GivingCat~FY15+YearsGiven+Event+NextDeg+Gender, data=dt_train, model=TRUE, trace=FALSE)
#summary(model.fit2d)
PrintModel(model.fit2d, FALSE)
Anova(model.fit2d, test="LR")
CMTr = PredAcc(dt_train, mod=model.fit2d)
CMTe = PredAcc(dt_test, mod=model.fit2d)
s = as.character(strsplit(as.character(model.fit2d$call$formula),"~")[3])
ac_table = rbind(ac_table, Accuracy(CMTr, CMTe, "model.fit2d",s))
```

##model.fit3a  
```{r message=FALSE, warning=FALSE}
#Below are the class of models without using prior year giving info
model.fit3a = multinom(FY16GivingCat~NextDeg+Event+Class+MS+Gender, data=dt_train, model=TRUE, trace=FALSE)
PrintModel(model.fit3a, FALSE); 
Anova(model.fit3a, test="LR")
CMTr = PredAcc(dt_train, mod=model.fit3a); CMTe = PredAcc(dt_test, mod=model.fit3a)
s = as.character(strsplit(as.character(model.fit3a$call$formula),"~")[3])
ac_table =  rbind(ac_table,Accuracy(CMTr, CMTe,"model.fit3a", s))
```
##model.fit4a
```{r message=FALSE, warning=FALSE}
library(ordinal)
model.fit4a = clm(FY16GivingCat~FY15+YearsGiven, data=dt_train, model=TRUE, trace=FALSE)
PrintModel(model.fit4a)
Anova(model.fit4a, test="F")
CMTr = PredAcc(dt_train, mod=model.fit4a)
CMTe = PredAcc(dt_test, mod=model.fit4a)
s = as.character(strsplit(as.character(model.fit4a$call$formula),"~")[3])
ac_table =  rbind(ac_table,Accuracy(CMTr, CMTe,"model.fit4a", s))
print("Model Accuracy Comparison")
print(ac_table)
```



#SECTION - 4 FINAL REMARKS
. Section 4: Final Remarks. After examining the data and using the data to build a predictive model,
what are your departing thoughts? What are the strengths and weaknesses in your analysis? Should
the administration trust your result? Are there subsample in your sample that your model did a bad
job in predicting their contribution behavior? If so, why? Are there other "things", a wish list, that you think can be used to improve your model? If so, what are they? Perhaps you can make a suggestion to the administration to collect those information in the future.